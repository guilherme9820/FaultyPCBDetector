{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MaskPCB.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"UFKk5VU7BJHA","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import time\n","import glob\n","import numpy as np\n","import cv2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from imgaug import augmenters as iaa\n","from sklearn.model_selection import train_test_split\n","import warnings\n","import random\n","from skimage.measure import find_contours\n","from sklearn import preprocessing\n","from matplotlib import patches\n","import webbrowser\n","from google.colab import drive\n","from google.colab.patches import cv2_imshow\n","\n","## Mounts Google Drive if it's not mounted yet\n","if not os.path.isdir('/content/drive'):\n","  drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p0RvQ5VarzXU","colab_type":"code","colab":{}},"source":["# Root directory of the project\n","ROOT_DIR = \"/content/drive/My Drive/Computer_Vision/MaskPCB/\"\n","\n","# Path to libraries\n","LIB_PATH = os.path.join(ROOT_DIR, 'libs/')\n","sys.path.append(LIB_PATH)\n","\n","# Path for DeepPCB dataset\n","PCB_DATASET = \"/content/drive/My Drive/Computer_Vision/MaskPCB/Dataset/\"\n","\n","# Directory to predictions\n","PCB_OUTPUTS = os.path.join(ROOT_DIR, \"Predictions/\")\n","if not os.path.exists(PCB_OUTPUTS):\n","    os.mkdir(PCB_OUTPUTS)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0MH9ptl3sLKa","colab_type":"code","outputId":"36f76004-db13-4ffd-fa51-2868d48d6e62","executionInfo":{"status":"ok","timestamp":1575456188578,"user_tz":180,"elapsed":8262,"user":{"displayName":"Guilherme Santos","photoUrl":"","userId":"09469490003834439085"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"source":["from mrcnn.config import Config\n","from mrcnn import utils\n","import mrcnn.model as modellib\n","from mrcnn import visualize\n","from mrcnn.model import log"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"P_EnVWZszEBd","colab_type":"code","colab":{}},"source":["def parse_data(dataframe):\n","\n","        # Creates a empty dictionary with image names as entry keys\n","    annotations = {os.path.join(PCB_DATASET, 'images', name): [] for name in dataframe['image']}\n","    image_paths = []\n","\n","    # Each row of dataframe contains two columns: images and annotations, the former\n","    # is image file name and later is the file name of a .txt containing the respective\n","    # labels of that image\n","    for _, rows in dataframe.iterrows():\n","\n","        # Appends the folder location of image\n","        image_paths.append(os.path.join(PCB_DATASET, 'images', rows['image']))\n","\n","        # The image labels are spatial localization (Bounding boxes) of the PCB defects and its\n","        # respective class (open, short, mousebite, spur, copper or pin-hole). Each line of\n","        # annotation file is given in the format x1,y1,x2,y2,type, where (x1, y1) and (x2, y2)\n","        # are top left corner and bottom right corner of the bounding box.\n","        with open(os.path.join(PCB_DATASET, 'annotations', rows['annotations']), 'r') as f:\n","            line = f.read().replace(' ', ',').splitlines()\n","        line = [[s for s in string.split(',')] for string in line]\n","        annotations[os.path.join(PCB_DATASET, 'images', rows['image'])] = line\n","\n","    return image_paths, annotations\n","\n","def get_latest_model(model, config):\n","\n","    dir_names = next(os.walk(model.model_dir))[1]\n","    key = config.NAME.lower()\n","    dir_names = filter(lambda f: f.startswith(key), dir_names)\n","    dir_names = sorted(dir_names)\n","\n","    if not dir_names:\n","        import errno\n","        raise FileNotFoundError(\n","            errno.ENOENT,\n","            \"Could not find model directory under {}\".format(self.model_dir))\n","\n","    fps = []\n","    # Pick last directory\n","    for d in dir_names:\n","        dir_name = os.path.join(model.model_dir, d)\n","        # Find the last checkpoint\n","        checkpoints = next(os.walk(dir_name))[2]\n","        checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n","        checkpoints = sorted(checkpoints)\n","        if not checkpoints:\n","            print('No weight files in {}'.format(dir_name))\n","        else:\n","            checkpoint = os.path.join(dir_name, checkpoints[-1])\n","            fps.append(checkpoint)\n","\n","    model_path = sorted(fps)[-1]\n","    print('Found model {}'.format(model_path))\n","\n","    return model_path\n","\n","def get_train_data(dataframe, ORIG_SIZE, val_size=0.2):\n","\n","    # get data from dataframe\n","    image_paths, annotations = parse_data(dataframe)\n","\n","    # Split into training e validation sets\n","    X_train, X_val = train_test_split(image_paths, test_size=val_size, random_state=42)\n","\n","    dataset_train = PCBDataset(X_train, annotations, ORIG_SIZE, ORIG_SIZE)\n","    dataset_train.prepare()\n","\n","    dataset_val = PCBDataset(X_val, annotations, ORIG_SIZE, ORIG_SIZE)\n","    dataset_val.prepare()\n","\n","    return dataset_train, dataset_val\n","\n","\n","def get_test_data(dataframe, ORIG_SIZE):\n","\n","    # get data from dataframe\n","    image_paths, annotations = parse_data(dataframe)\n","\n","    dataset = PCBDataset(image_paths, annotations, ORIG_SIZE, ORIG_SIZE)\n","    dataset.prepare()\n","\n","    return dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dtJ7k2EkTyyq","colab_type":"code","colab":{}},"source":["def get_colors_for_class_ids(class_ids):\n","\n","    # Define a different color for each defect class\n","    # 1-open, 2-short, 3-mousebite, 4-spur, 5-copper, 6-pin-hole\n","    COLORS = {1: (.941, .204, .204),\n","              2: (.204, .941, .941),\n","              3: (.204, .941, .204),\n","              4: (.204, .204, .941),\n","              5: (1., .498, .314),\n","              6: (.500, .300, .750)}\n","    \n","    colors = []\n","    for class_id in class_ids:\n","        if class_id in COLORS:\n","            colors.append(COLORS[class_id])\n","\n","    return colors\n","\n","def visualize_predictions(test_image, image_id, classes, save=True, display=True):\n","\n","    # Load configurations for inference\n","    inference_config = InferenceConfig()\n","\n","    # Recreate the model in inference mode\n","    model = modellib.MaskRCNN(mode='inference',\n","                              config=inference_config,\n","                              model_dir=ROOT_DIR)\n","\n","    model_path = get_latest_model(model, inference_config)\n","\n","    # Load trained weights (fill in path to trained weights here)\n","    assert model_path != \"\", \"Provide path to trained weights\"\n","    print(\"Loading weights from \", model_path)\n","    model.load_weights(model_path, by_name=True)\n","\n","    fig = plt.figure(figsize=(10, 30))\n","    plt.subplot(1, 1, 1)\n","        \n","    results = model.detect([test_image], verbose=1)\n","    r = results[0]\n","    display_instances(test_image, r['rois'], r['masks'], r['class_ids'],\n","                        classes, r['scores'],\n","                        colors=get_colors_for_class_ids(r['class_ids']), ax=fig.axes[-1], title='predicted', text_color=(.6, .6, 0))\n","    if save:\n","        plt.savefig(os.path.join(PCB_OUTPUTS, f\"pred_{image_id}.png\"), bbox_inches=\"tight\")\n","        # plt.close('all')\n","\n","    if display:\n","        plt.show()\n","    else:\n","        plt.close('all')\n","\n","def display_instances(image, boxes, masks, class_ids, class_names,\n","                      scores=None, title=\"\",\n","                      figsize=(16, 16), ax=None,\n","                      show_mask=True, show_bbox=True,\n","                      colors=None, captions=None, text_color=None):\n","    \"\"\"\n","    boxes: [num_instance, (y1, x1, y2, x2, class_id)] in image coordinates.\n","    masks: [height, width, num_instances]\n","    class_ids: [num_instances]\n","    class_names: list of class names of the dataset\n","    scores: (optional) confidence scores for each box\n","    title: (optional) Figure title\n","    show_mask, show_bbox: To show masks and bounding boxes or not\n","    figsize: (optional) the size of the image\n","    colors: (optional) An array or colors to use with each object\n","    captions: (optional) A list of strings to use as captions for each object\n","    \"\"\"\n","    # Number of instances\n","\n","    def apply_mask(image, mask, color, alpha=0.5):\n","        \"\"\"Apply the given mask to the image.\n","        \"\"\"\n","        for c in range(3):\n","            image[:, :, c] = np.where(mask == 1,\n","                                      image[:, :, c] *\n","                                      (1 - alpha) + alpha * color[c] * 255,\n","                                      image[:, :, c])\n","        return image\n","\n","    N = boxes.shape[0]\n","    if not N:\n","        print(\"\\n*** No instances to display *** \\n\")\n","    else:\n","        assert boxes.shape[0] == masks.shape[-1] == class_ids.shape[0]\n","\n","    # If no axis is passed, create one and automatically call show()\n","    auto_show = False\n","    if not ax:\n","        _, ax = plt.subplots(1, figsize=figsize)\n","        auto_show = True\n","\n","    # Show area outside image boundaries.\n","    height, width = image.shape[:2]\n","    ax.set_ylim(height + 10, -10)\n","    ax.set_xlim(-10, width + 10)\n","    ax.axis('off')\n","    ax.set_title(title)\n","\n","    masked_image = image.astype(np.uint32).copy()\n","    for i in range(N):\n","        color = colors[i]\n","\n","        # Bounding box\n","        if not np.any(boxes[i]):\n","            # Skip this instance. Has no bbox. Likely lost in image cropping.\n","            continue\n","        y1, x1, y2, x2 = boxes[i]\n","        if show_bbox:\n","            p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n","                                  alpha=0.7, linestyle=\"dashed\",\n","                                  edgecolor=color, facecolor='none')\n","            ax.add_patch(p)\n","\n","        # Label\n","        if not captions:\n","            class_id = class_ids[i]\n","            score = scores[i] if scores is not None else None\n","            label = class_names[class_id]\n","            caption = \"{} {:.3f}\".format(label, score) if score else label\n","        else:\n","            caption = captions[i]\n","        ax.text(x1, y1 + 8, caption,\n","                color=text_color, size=11, backgroundcolor=\"none\")\n","\n","        # Mask\n","        mask = masks[:, :, i]\n","        if show_mask:\n","            masked_image = apply_mask(masked_image, mask, color)\n","\n","        # Mask Polygon\n","        # Pad to ensure proper polygons for masks that touch image edges.\n","        padded_mask = np.zeros(\n","            (mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n","        padded_mask[1:-1, 1:-1] = mask\n","        contours = find_contours(padded_mask, 0.5)\n","        for verts in contours:\n","            # Subtract the padding and flip (y, x) to (x, y)\n","            verts = np.fliplr(verts) - 1\n","            p = patches.Polygon(verts, facecolor=\"none\", edgecolor=color)\n","            ax.add_patch(p)\n","    ax.imshow(masked_image.astype(np.uint8))\n","    if auto_show:\n","        plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Pj7ISdgE_AE","colab_type":"code","colab":{}},"source":["class PCBDataset(utils.Dataset):\n","\n","    def __init__(self, image_paths, img_annotations, height, width):\n","        super().__init__(self)\n","        \"\"\"Get data from dataframe.\n","            Dataframe must have two columns: the first one must contain the image filenames\n","            and the second one must contain the name files (.txt) with the groundtruth bounding boxes \n","        \"\"\"\n","        # Add classes\n","        self.add_class(\"deeppcb\", 1, \"open\")\n","        self.add_class(\"deeppcb\", 2, \"short\")\n","        self.add_class(\"deeppcb\", 3, \"mousebite\")\n","        self.add_class(\"deeppcb\", 4, \"spur\")\n","        self.add_class(\"deeppcb\", 5, \"copper\")\n","        self.add_class(\"deeppcb\", 6, \"pin-hole\")\n","\n","        # Add images and labels\n","        for i, fp in enumerate(image_paths):\n","            annotations = img_annotations[fp]\n","            self.add_image('deeppcb', image_id=i, path=fp,\n","                           annotations=annotations, orig_height=height, orig_width=width)\n","\n","    def image_path(self, image_id):\n","        # Returning folder location of image given its id\n","        info = self.image_info[image_id]\n","        return info['path']\n","\n","    def load_image(self, image_id):\n","        \"\"\"Load the specified image and return a [H,W,3] Numpy array.\n","        \"\"\"\n","        NORMALIZERS = {'min_max': preprocessing.MinMaxScaler(),\n","                       'max_abs': preprocessing.MaxAbsScaler(),\n","                       'standard': preprocessing.StandardScaler(),\n","                       'robust': preprocessing.RobustScaler(),\n","                       'normalizer': preprocessing.Normalizer(),\n","                       'quantile': preprocessing.QuantileTransformer(),\n","                       'power': preprocessing.PowerTransformer()}\n","\n","        # Load image\n","        image = cv2.imread(self.image_info[image_id]['path'], cv2.IMREAD_GRAYSCALE)\n","        _, image = cv2.threshold(image, 100, 255, cv2.THRESH_BINARY)\n","        \n","        # Performs standardization\n","        image = NORMALIZERS['power'].fit_transform(image)\n","        \n","        # If grayscale. Convert to RGB for consistency.\n","        if image.ndim != 3:\n","            image = np.stack((image,) * 3, axis=-1)\n","\n","        return image.astype(np.float32)\n","\n","    def load_mask(self, image_id):\n","        info = self.image_info[image_id]\n","        annotations = info['annotations']\n","        count = len(annotations)\n","\n","        # Generates masks if exist at least one bounding box\n","        if count == 0:\n","            mask = np.zeros((info['orig_height'], info['orig_width'], 1), dtype=np.uint8)\n","            class_ids = np.zeros((1,), dtype=np.int32)\n","        else:\n","            mask = np.zeros((info['orig_height'], info['orig_width'], count), dtype=np.uint8)\n","            class_ids = np.zeros((count,), dtype=np.int32)\n","            for i, a in enumerate(annotations):\n","                mask_instance = mask[:, :, i].copy()\n","                cv2.rectangle(mask_instance, (int(a[0]), int(a[1])), (int(a[2]), int(a[3])), 255, -1)\n","                mask[:, :, i] = mask_instance\n","                class_ids[i] = int(a[4])\n","\n","        return mask.astype(np.bool), class_ids.astype(np.int32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TIFdvUX3BsGt","colab_type":"code","colab":{}},"source":["class ArchitectureConfig(Config):\n","    \"\"\"Configuration for training on the toy shapes dataset.\n","    Derives from the base Config class and overrides values specific\n","    to the toy shapes dataset.\n","    \"\"\"\n","    # Give the configuration a recognizable name\n","    NAME = \"deeppcb\"\n","\n","    # Train on 1 GPU and 2 images per GPU.\n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 2\n","\n","    # Number of classes (including background)\n","    NUM_CLASSES = 1 + 6  # background + 6 classes\n","\n","    # Image dimensions\n","    IMAGE_MIN_DIM = 640\n","    IMAGE_MAX_DIM = 640\n","\n","    RPN_ANCHOR_SCALES = (32, 64, 128, 256)  # anchor side in pixels\n","\n","    # Maximum number of RoIs per proposal\n","    TRAIN_ROIS_PER_IMAGE = 100\n","\n","    # Core of the architecture\n","    BACKBONE = 'resnet50'\n","\n","    DETECTION_MIN_CONFIDENCE = 0.9\n","    DETECTION_NMS_THRESHOLD = 0.1\n","\n","class InferenceConfig(ArchitectureConfig):\n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 1\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gyJ9DSd4CJAm","colab_type":"code","colab":{}},"source":["def train_model(dataset_train, dataset_val):\n","\n","    config = ArchitectureConfig()\n","    # config.display()\n","\n","    model = modellib.MaskRCNN(mode='training', config=config, model_dir=ROOT_DIR)\n","\n","    model_path = get_latest_model(model, config)\n","\n","    # Load trained weights (fill in path to trained weights here)\n","    assert model_path != \"\", \"Provide path to trained weights\"\n","    print(\"Loading weights from \", model_path)\n","    model.load_weights(model_path, by_name=True)\n","\n","    augmentation = iaa.SomeOf((0, 1), [\n","        iaa.Fliplr(0.5),\n","        iaa.Affine(\n","            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n","            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n","            rotate=(-25, 25),\n","            shear=(-8, 8)\n","        ),\n","        iaa.Multiply((0.9, 1.1))\n","    ])\n","\n","    NUM_EPOCHS = 100\n","\n","    warnings.filterwarnings(\"ignore\")\n","    model.train(dataset_train, dataset_val,\n","                learning_rate=config.LEARNING_RATE,\n","                epochs=NUM_EPOCHS,\n","                layers='all',\n","                augmentation=augmentation)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N9_ThFHRG4gE","colab_type":"code","outputId":"ed0b3776-ed5a-402c-e1b4-bb98c067e328","executionInfo":{"status":"error","timestamp":1575456265500,"user_tz":180,"elapsed":85144,"user":{"displayName":"Guilherme Santos","photoUrl":"","userId":"09469490003834439085"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["training = False\n","image_id = 0\n","\n","CROP_PATH = \"/content/drive/My Drive/Computer_Vision/data_notebooks/crop_img_verso_defeito/\"\n","\n","if training:\n","    train_dataframe = pd.read_csv(os.path.join(PCB_DATASET, \"train_data.csv\"))\n","\n","    dataset_train, dataset_val = get_train_data(train_dataframe, 640)\n","\n","    train_model(dataset_train, dataset_val)\n","else:\n","\n","    test_dataframe = pd.read_csv(os.path.join(PCB_DATASET, \"test_data.csv\"))\n","\n","    dataset_test = get_test_data(test_dataframe, 640)\n","\n","    classes = dataset_test.class_names\n","    # test_image = dataset_test.load_image(image_id)\n","\n","    test_images = next(os.walk(CROP_PATH))[2]\n","    test_images = filter(lambda f: f.startswith(\"placa\"), test_images)\n","    test_images = sorted(test_images)\n","\n","    for i, test_image in enumerate(test_images):\n","        test_image = cv2.imread(os.path.join(CROP_PATH, test_image) , cv2.IMREAD_GRAYSCALE)\n","        if test_image.shape == (640, 640):\n","            _, test_image = cv2.threshold(test_image, 100, 255, cv2.THRESH_BINARY)\n","            if len(test_image.shape) < 3: # If image is single channel\n","                test_image = np.stack((test_image,)*3, axis=-1)\n","\n","            visualize_predictions(test_image, i, classes, save=True, display=False)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2139: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Computer_Vision/MaskPCB/libs/mrcnn/model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Computer_Vision/MaskPCB/libs/mrcnn/model.py:399: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/drive/My Drive/Computer_Vision/MaskPCB/libs/mrcnn/model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","WARNING:tensorflow:From /content/drive/My Drive/Computer_Vision/MaskPCB/libs/mrcnn/model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Computer_Vision/MaskPCB/libs/mrcnn/model.py:722: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Computer_Vision/MaskPCB/libs/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","Found model /content/drive/My Drive/Computer_Vision/MaskPCB/deeppcb20191007T1315/mask_rcnn_deeppcb_0100.h5\n","Loading weights from  /content/drive/My Drive/Computer_Vision/MaskPCB/deeppcb20191007T1315/mask_rcnn_deeppcb_0100.h5\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","Re-starting from epoch 100\n","Processing 1 images\n","image                    shape: (640, 640, 3)         min:    0.00000  max:  255.00000  uint8\n","molded_images            shape: (1, 640, 640, 3)      min: -123.70000  max:  151.10000  float64\n","image_metas              shape: (1, 19)               min:    0.00000  max:  640.00000  int64\n","anchors                  shape: (1, 102000, 4)        min:   -0.28329  max:    1.23321  float32\n"],"name":"stdout"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-d93b59bc8b0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mvisualize_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-cdc5e9401666>\u001b[0m in \u001b[0;36mvisualize_predictions\u001b[0;34m(test_image, image_id, classes, save, display)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     display_instances(test_image, r['rois'], r['masks'], r['class_ids'],\n","\u001b[0;32m/content/drive/My Drive/Computer_Vision/MaskPCB/libs/mrcnn/model.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self, images, verbose)\u001b[0m\n\u001b[1;32m   2522\u001b[0m         \u001b[0;31m# Run object detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmrcnn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2524\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmolded_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_metas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2525\u001b[0m         \u001b[0;31m# Process detections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1399\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     def train_on_batch(self, x, y,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: indices[216] = 102002 is not in [0, 102000)\n\t [[{{node ROI/GatherV2_2}}]]"]},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAlsAAAaCCAYAAAASuziFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzcX6hv55nQ8eeZxiLU0bnoESRJnV5k\nqEGEGTd1YC4sWCHtRXIhSAMiyjC5sYPgIFSUKvVKBS+E+CeCDApOiV5IwEgEqQhiJaeMFpNSOcQ/\nSRR6HIe5GbQWXi/OGdmeSXr2JPs77U4/H9iw17ve31rP5Ze11/7tOWcAAGj80Pd6AACADzKxBQAQ\nElsAACGxBQAQElsAACGxBQAQemhs7e7f291v7e5/eJfzu7t/Y3fv7O7Xd/cnrn9MAICb6SpPtn5+\nZp76Luc/MzNP3P95bmb+1vsfCwDgg+GhsXXO+Vcz8z+/y5ZnZubvn3u+OjM/sru/67oGBAC4ya7j\nna1HZ+bNS8dv3V8DAPiB98hv5s1297m596fG+chHPvL7P/GJT/xm3h4A4D352te+9j/OObfey2ev\nI7benpnHLx0/dn/t1znnvDAzL8zMXFxcnNu3b1/D7QEAWrv7X97rZ6/jz4gvzcwfv/9fiT85M79y\nzvnv13BdAIAb76FPtnb3F2bmUzPz0d19a2b+4sz8lpmZc87fnpmXZ+azM3NnZn51Zv5kNSwAwE3z\n0Ng65zz7kPNnZv7UtU0EAPAB4hvkAQBCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCV4qt3X1qd7+5u3d29wvvcP5ju/uV3f3F3f367n72+kcFALh5\nHhpbu/uhmXl+Zj4zM0/OzLO7++QD2/7CzLx4zvnxmfnczPzN6x4UAOAmusqTrU/OzJ1zzhvnnG/P\nzJdn5pkH9pyZ+e33f/8dM/Pfrm9EAICb65Er7Hl0Zt68dPzWzPyBB/b8pZn557v7szPzkZn59LVM\nBwBww13XC/LPzszPn3Mem5nPzsw/2N1fd+3dfW53b+/u7bt3717TrQEAvn9dJbbenpnHLx0/dn/t\nsp+emRdnZs45/2ZmfuvMfPTBC51zXjjnXJxzLm7duvXeJgYAuEGuEluvzswTu/vx3f3w3HsB/qUH\n9vzXmflDMzO7+3vmXmx5dAUA/MB7aGydc74zM5+fmVdm5htz778OX9vdL+3u0/e3/dzM/Mzu/vuZ\n+YWZ+RPnnFMNDQBwU1zlBfk557w8My8/sPbFS7+/PjM/db2jAQDcfL5BHgAgJLYAAEJiCwAgJLYA\nAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJi\nCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAg\nJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYA\nAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJi\nCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAg\nJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYA\nAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJi\nCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAg\nJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYA\nAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJi\nCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAg\nJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYA\nAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJi\nCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAg\nJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYA\nAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJi\nCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAg\nJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYA\nAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJi\nCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAg\nJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYA\nAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJi\nCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAg\nJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYA\nAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJiCwAgJLYAAEJXiq3dfWp3v7m7\nd3b3C++y54/u7uu7+9ru/sPrHRMA4GZ65GEbdvdDM/P8zPzhmXlrZl7d3ZfOOa9f2vPEzPy5mfmp\nc84v7+7vrAYGALhJrvJk65Mzc+ec88Y559sz8+WZeeaBPT8zM8+fc355Zuac863rHRMA4Ga6Smw9\nOjNvXjp+6/7aZT82Mz+2u/96d7+6u0+904V297ndvb27t+/evfveJgYAuEGu6wX5R2bmiZn51Mw8\nOzN/d3d/5MFN55wXzjkX55yLW7duXdOtAQC+f10ltt6emccvHT92f+2yt2bmpXPO/znn/KeZ+Y9z\nL74AAH6gXSW2Xp2ZJ3b347v74Zn53My89MCefzL3nmrN7n507v1Z8Y1rnBMA4EZ6aGydc74zM5+f\nmVdm5hsz8+I557Xd/dLuPn1/2ysz80u7+/rMfGVm/uw555eqoQEAboo953xPbnxxcXFu3779Pbk3\nAMBvxO5+7Zxz8V4+6xvkAQBCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2\nAABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABC\nYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsAICS2AABCYgsA\nICS2AABCYgsAICS2AABCV4qt3X1qd7+5u3d29wvfZd8f2d2zuxfXNyIAwM310Nja3Q/NzPMz85mZ\neXJmnt3dJ99h3w/PzJ+emX973UMCANxUV3my9cmZuXPOeeOc8+2Z+fLMPPMO+/7yzPyVmflf1zgf\nAMCNdpXYenRm3rx0/Nb9tf9nd39iZh4/5/zT73ah3X1ud2/v7u27d+/+hocFALhp3vcL8rv7QzPz\n12fm5x6295zzwjnn4pxzcevWrfd7awCA73tXia23Z+bxS8eP3V/7NT88M793Zv7l7v7nmfnJmXnJ\nS/IAAFeLrVdn5ond/fjufnhmPjczL/3ayXPOr5xzPnrO+dFzzo/OzFdn5ulzzu1kYgCAG+ShsXXO\n+c7MfH5mXpmZb8zMi+ec13b3S7v7dD0gAMBN9shVNp1zXp6Zlx9Y++K77P3U+x8LAOCDwTfIAwCE\nxBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYA\nQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhs\nAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCE\nxBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYA\nQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhs\nAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCE\nxBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYA\nQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhs\nAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCE\nxBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYA\nQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhs\nAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCE\nxBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYA\nQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhs\nAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCE\nxBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYA\nQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhs\nAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCE\nxBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYA\nQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhs\nAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCE\nxBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYA\nQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhs\nAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCE\nxBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCExBYAQEhsAQCErhRb\nu/vU7n5zd+/s7hfe4fyf2d3Xd/fru/svdvd3X/+oAAA3z0Nja3c/NDPPz8xnZubJmXl2d598YNsv\nzszFOef3zcw/npm/et2DAgDcRFd5svXJmblzznnjnPPtmfnyzDxzecM55yvnnF+9f/jVmXnsescE\nALiZrhJbj87Mm5eO37q/9m5+emb+2fsZCgDgg+KR67zY7v6xmbmYmT/4Luefm5nnZmY+9rGPXeet\nAQC+L13lydbbM/P4pePH7q/9f3b30zPz52fm6XPO/36nC51zXjjnXJxzLm7duvVe5gUAuFGuEluv\nzswTu/vx3f3wzHxuZl66vGF3f3xm/s7cC61vXf+YAAA300Nj65zznZn5/My8MjPfmJkXzzmv7e6X\ndvfp+9v+2sz8tpn5R7v773b3pXe5HADAD5QrvbN1znl5Zl5+YO2Ll37/9DXPBQDwgeAb5AEAQmIL\nACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAk\ntgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAA\nQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmIL\nACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAk\ntgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAA\nQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmIL\nACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAk\ntgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAA\nQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmIL\nACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAk\ntgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAA\nQmILACAktgAAQmILACAktgyB9aEAAAgoSURBVAAAQmILACAktgAAQmILACAktgAAQmILACAktgAA\nQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmILACAktgAAQmIL\nACAktgAAQmILACAktgAAQmILACAktgAAQmIL+L/t3VuI7QUVx/HfSrs8ZAadlyjLII3MAkPC6KHA\nCPVBH7qQIGVIPhXdCIqiop4qKgjsYiRW0MV6iAMZPpQhREqCICoUBws7FdjFfBEta/WwN3E4jmf+\nZzhrz2z7fODA7Jk9exYs9pzv/PflD8AgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACD\nxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACD\nxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACD\nxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACD\nxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACD\nxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACD\nxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACD\nxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACD\nxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACD\nxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACD\nxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACD\nxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIPEFgDAILEFADBIbAEADBJbAACD\nxBYAwCCxBQAwSGwBAAwSWwAAg8QWAMAgsQUAMEhsAQAMElsAAIMWxVZVXVJVv6mqI1X1kR2+/syq\n+sH663dU1dmnelAAgG20a2xV1WlJrktyaZLzklxZVecdd7VrkjzU3S9N8qUknz3VgwIAbKMlR7Ze\nk+RId9/f3f9M8v0kVxx3nSuSfGv98Y+SXFxVderGBADYTkti6wVJ/nDM5aPrz+14ne5+PMnDSZ53\nKgYEANhmp2/yh1XVtUmuXV98rKru2eTP55Q6lOSv+z0Ee2J3283+tpfdbbeX7fUbl8TWH5Ocdczl\nF64/t9N1jlbV6UnOTPK342+ou69Pcn2SVNWd3X3hXoZm/9nf9rK77WZ/28vutltV3bnX713yMOKv\nk5xTVS+pqmckeXuSw8dd53CSd64/fkuSn3d373UoAICnil2PbHX341X1niS3JDktyQ3dfW9VfTrJ\nnd19OMk3k3ynqo4k+XtWQQYA8H9v0XO2uvvmJDcf97lPHPPxo0neepI/+/qTvD4Hi/1tL7vbbva3\nvexuu+15f+XRPgCAOU7XAwAwaDy2nOpney3Y3Qer6r6quruqflZVL96POdnZbvs75npvrqquKq+S\nOkCW7K+q3ra+D95bVd/d9IzsbMHvzhdV1a1Vddf69+dl+zEnT1RVN1TVg0/21lS18uX1bu+uqlcv\nud3R2HKqn+21cHd3Jbmwu1+V1ZkDPrfZKXkyC/eXqjojyfuS3LHZCTmRJfurqnOSfDTJ67r7FUne\nv/FBeYKF972PJ7mpuy/I6gVlX9nslJzAjUkuOcHXL01yzvrftUm+uuRGp49sOdXP9tp1d919a3c/\nsr54e1bvwcbBsOS+lySfyeoPnEc3ORy7WrK/dye5rrsfSpLufnDDM7KzJbvrJM9Zf3xmkj9tcD5O\noLtvy+pdFZ7MFUm+3Su3J3luVT1/t9udji2n+tleS3Z3rGuS/HR0Ik7GrvtbH/4+q7t/ssnBWGTJ\n/e/cJOdW1S+r6vaqOtFf42zOkt19KslVVXU0q1f6v3czo3EKnOz/jUk2fLoenpqq6qokFyZ5/X7P\nwjJV9bQkX0xy9T6Pwt6dntVDGW/I6qjybVX1yu7+x75OxRJXJrmxu79QVa/N6n0qz+/u/+z3YMyY\nPrJ1Mqf6yYlO9cPGLdldquqNST6W5PLufmxDs7G73fZ3RpLzk/yiqn6f5KIkhz1J/sBYcv87muRw\nd/+ru3+X5LdZxRf7a8nurklyU5J096+SPCur8yZy8C36v/F407HlVD/ba9fdVdUFSb6eVWh5vsjB\ncsL9dffD3X2ou8/u7rOzes7d5d2953N/cUot+d3546yOaqWqDmX1sOL9mxySHS3Z3QNJLk6Sqnp5\nVrH1l41OyV4dTvKO9asSL0rycHf/ebdvGn0Y0al+ttfC3X0+ybOT/HD9moYHuvvyfRua/1m4Pw6o\nhfu7Jcmbquq+JP9O8uHu9qjAPlu4uw8l+UZVfSCrJ8tf7SDDwVBV38vqj5hD6+fUfTLJ05Oku7+W\n1XPsLktyJMkjSd616HbtFwBgjneQBwAYJLYAAAaJLQCAQWILAGCQ2AIAGCS2AAAGiS0AgEFiCwBg\n0H8B7Ty2aXz0plYAAAAASUVORK5CYII=\n","text/plain":["<Figure size 720x2160 with 1 Axes>"]},"metadata":{"tags":[]}}]}]}